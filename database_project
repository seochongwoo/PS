from typing import Optional, Tuple
import sqlite3
import numpy as np  # 벡터 계산을 위해 추가
import hashlib  # query_hash 생성을 위해 추가
import datetime  # expires_at 계산을 위해 추가
import time  # 지수 백오프를 위해 추가
import google.generativeai as genai  # 올바른 임포트 방식
from sklearn.feature_extraction.text import TfidfVectorizer  # 응답 비교를 위한 TF-IDF 추가
from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity  # scikit-learn cosine similarity

# Gemini 클라이언트 객체를 저장할 전역 변수 및 모델 이름 설정
GEMINI_CLIENT: Optional[genai.GenerativeModel] = None
# RAG Context Retrieval을 위한 임베딩 모델 (768 차원 예상)
GEMINI_EMBEDDING_MODEL = 'models/text-embedding-004'
# 텍스트 생성을 위한 LLM 모델 (지침에 따른 최신 모델)
LLM_MODEL_NAME = 'gemini-2.5-flash-preview-09-2025'
_INITIALIZED_ATTEMPTED = False
MAX_RETRIES = 5 # API 호출 최대 재시도 횟수

def get_gemini_client() -> Optional[genai.GenerativeModel]:
    """
    Gemini 클라이언트를 초기화하거나 반환합니다.
    API Key는 Canvas 환경을 위해 빈 문자열로 설정합니다.
    """
    global GEMINI_CLIENT, _INITIALIZED_ATTEMPTED
    
    if GEMINI_CLIENT is not None:
        return GEMINI_CLIENT
    
    if _INITIALIZED_ATTEMPTED:
        return None
    
    _INITIALIZED_ATTEMPTED = True
    
    api_key = ''  # API 키 설정
    
    try:
        genai.configure(api_key=api_key)  # API 키 직접 설정
        # LLM_MODEL_NAME을 사용하여 텍스트 모델 초기화
        GEMINI_CLIENT = genai.GenerativeModel(LLM_MODEL_NAME)
        print(f"✅ Gemini 클라이언트 초기화 성공. 모델: {LLM_MODEL_NAME}")
        return GEMINI_CLIENT
    except Exception as e:
        print(f"❌ 초기화 오류: {e}")
        return None

# 데이터베이스 생성 및 테이블 생성 함수
def create_database_and_tables(db_name: str = "llm_rag_db.sqlite"):
    """
    SQLite 데이터베이스를 생성하고, 보고서 스키마에 기반한 테이블을 생성합니다.
    """
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    
    # Foreign Key 강제 적용
    cur.execute("PRAGMA foreign_keys = ON")
    
    # 테이블 생성 SQL (보고서 스키마 기반, 벡터 조정)
    tables_sql = """
    -- User 테이블
    CREATE TABLE IF NOT EXISTS users (
        user_id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        email TEXT UNIQUE NOT NULL,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
    );
    CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);

    -- Subject 테이블
    CREATE TABLE IF NOT EXISTS subjects (
        subject_id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT UNIQUE NOT NULL,
        description TEXT
    );

    -- Chatlog 테이블 (embedding_vec을 BLOB으로)
    CREATE TABLE IF NOT EXISTS chatlogs (
        log_id INTEGER PRIMARY KEY AUTOINCREMENT,
        user_id INTEGER NOT NULL,
        subject_id INTEGER,
        role TEXT NOT NULL,  -- 'user' or 'assistant'
        message TEXT NOT NULL,
        embedding_vec BLOB,  -- 벡터 데이터 (np.float32)
        is_target BOOLEAN DEFAULT FALSE,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
        FOREIGN KEY (subject_id) REFERENCES subjects(subject_id) ON DELETE SET NULL
    );
    CREATE INDEX IF NOT EXISTS idx_chatlogs_user ON chatlogs(user_id);
    CREATE INDEX IF NOT EXISTS idx_chatlogs_time ON chatlogs(created_at);
    CREATE INDEX IF NOT EXISTS idx_chatlogs_user_target ON chatlogs(user_id, is_target);

    -- Document 테이블
    CREATE TABLE IF NOT EXISTS documents (
        doc_id INTEGER PRIMARY KEY AUTOINCREMENT,
        user_id INTEGER NOT NULL,
        content TEXT NOT NULL,
        source_url TEXT,
        embedding_vec BLOB,
        uploaded_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
    );
    CREATE INDEX IF NOT EXISTS idx_documents_user ON documents(user_id);

    -- Feedback 테이블
    CREATE TABLE IF NOT EXISTS feedbacks (
        feedback_id INTEGER PRIMARY KEY AUTOINCREMENT,
        log_id INTEGER NOT NULL,
        rating INTEGER NOT NULL CHECK (rating BETWEEN 1 AND 5),
        comment TEXT,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (log_id) REFERENCES chatlogs(log_id) ON DELETE CASCADE
    );
    CREATE INDEX IF NOT EXISTS idx_feedbacks_log ON feedbacks(log_id);

    -- UserPreferences 테이블
    CREATE TABLE IF NOT EXISTS user_preferences (
        pref_id INTEGER PRIMARY KEY AUTOINCREMENT,
        user_id INTEGER NOT NULL,
        preference_type TEXT NOT NULL,
        value TEXT NOT NULL,
        UNIQUE (user_id, preference_type),
        FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE
    );
    CREATE INDEX IF NOT EXISTS idx_preferences_user ON user_preferences(user_id);

    -- Cache 테이블
    CREATE TABLE IF NOT EXISTS caches (
        cache_id INTEGER PRIMARY KEY AUTOINCREMENT,
        user_id INTEGER,
        query_hash TEXT UNIQUE NOT NULL,  -- SHA-256 해시, UNIQUE for duplicate prevention
        response TEXT NOT NULL,
        expires_at DATETIME NOT NULL,
        FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE SET NULL
    );
    CREATE INDEX IF NOT EXISTS idx_caches_expires ON caches(expires_at);
    """
    
    # SQL 문을 분할하여 실행 (SQLite에서 여러 문 실행을 위해)
    for statement in tables_sql.split(';'):
        if statement.strip():
            cur.execute(statement)
    
    conn.commit()
    print("✅ 모든 테이블 생성 및 인덱스 설정 완료. 데이터 무결성과 정규화 적용됨.")
    
    cur.close()
    conn.close()

# 임베딩 생성 헬퍼 함수
def get_embedding(text: str) -> np.ndarray:
    """
    Gemini API를 사용하여 텍스트 임베딩 벡터(np.float32)를 생성합니다.
    """
    try:
        # get_gemini_client()를 호출하여 API 키가 설정되도록 보장
        get_gemini_client()
        
        result = genai.embed_content(
            model=GEMINI_EMBEDDING_MODEL,
            content=text,
            task_type="RETRIEVAL_QUERY" # 검색 쿼리용 task_type 사용
        )
        # np.float32로 명시적으로 변환하여 바이트 표현 일관성 유지
        embedding_vector = np.array(result['embedding'], dtype=np.float32)
        # 디버깅을 위해 차원 출력
        print(f"DEBUG: 텍스트 임베딩 생성 (모델: {GEMINI_EMBEDDING_MODEL}) 차원: {embedding_vector.shape[0]}")
        return embedding_vector
    except Exception as e:
        print(f"❌ 임베딩 생성 오류: {e}")
        # 임베딩 실패 시 768차원 0 벡터 반환 (실제 사용 시에는 예외 처리 필요)
        return np.zeros(768, dtype=np.float32)

# 코사인 유사도 계산
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """두 벡터 간의 코사인 유사도를 계산합니다."""
    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
        return 0.0
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 사용자 선호도 가져오기 (user_preferences 테이블 사용)
def get_user_preferences(db_name: str, user_id: int) -> str:
    """사용자 선호도를 문자열로 반환합니다."""
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    cur.execute("""
    SELECT preference_type, value FROM user_preferences WHERE user_id = ?
    """, (user_id,))
    prefs = cur.fetchall()
    cur.close()
    conn.close()
    return ", ".join([f"{ptype}: {pvalue}" for ptype, pvalue in prefs]) if prefs else "기본 (간결)"

# 캐시 확인 및 저장 (caches 테이블 사용)
def get_cached_response(db_name: str, user_id: int, query: str) -> Optional[str]:
    """쿼리 해시 기반으로 유효한 캐시 응답을 검색합니다."""
    query_hash = hashlib.sha256(query.encode()).hexdigest()
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    cur.execute("""
    SELECT response FROM caches 
    WHERE user_id = ? AND query_hash = ? AND expires_at > DATETIME('now')
    """, (user_id, query_hash))
    result = cur.fetchone()
    cur.close()
    conn.close()
    return result[0] if result else None

def store_cached_response(db_name: str, user_id: int, query: str, response: str):
    """응답을 해시하여 캐시 테이블에 저장합니다."""
    query_hash = hashlib.sha256(query.encode()).hexdigest()
    # 1시간 후 만료되도록 설정
    expires_at = (datetime.datetime.now() + datetime.timedelta(hours=1)).strftime('%Y-%m-%d %H:%M:%S')
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    # INSERT OR REPLACE를 사용하여 중복 쿼리 시 갱신
    cur.execute("""
    INSERT OR REPLACE INTO caches (user_id, query_hash, response, expires_at) 
    VALUES (?, ?, ?, ?)
    """, (user_id, query_hash, response, expires_at))
    conn.commit()
    cur.close()
    conn.close()

# DB에서 관련 컨텍스트 검색 (Efficient Query Design 반영)
def retrieve_context(db_name: str, user_id: int, query_embedding: np.ndarray, subject_id: Optional[int] = None, top_k: int = 5) -> Tuple[str, str]:
    """
    최근 대화 로그와 벡터 유사도 기반 관련 문서/로그를 검색하여 반환합니다.
    """
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    
    # 1. 최근 5턴 로그 검색 (순서대로)
    log_query = """
    SELECT c.role, c.message 
    FROM chatlogs c
    WHERE c.user_id = ? AND (c.subject_id = ? OR ? IS NULL)
    ORDER BY c.created_at DESC LIMIT 5;
    """
    cur.execute(log_query, (user_id, subject_id, subject_id))
    logs = cur.fetchall()
    # 오래된 순서대로 정렬하여 컨텍스트 생성
    log_context = "\n".join([f"{role}: {message}" for role, message in reversed(logs)])
    
    # 2. 벡터 검색 (문서 및 전체 로그)
    expected_dim = query_embedding.shape[0]
    
    def safe_load_vector(vec_blob, source_type):
        if not vec_blob:
            return None
        # 저장된 바이트를 np.float32 배열로 변환
        vec = np.frombuffer(vec_blob, dtype=np.float32)
        if vec.shape[0] != expected_dim:
            # 디버깅 출력: 차원 불일치 메시지
            print(f"⚠️ 차원 불일치 감지 ({source_type}): 기대 {expected_dim}, 실제 {vec.shape[0]} → 스킵")
            return None
        return vec
    
    all_scores = []
    
    # 2a. 문서 검색
    cur.execute("SELECT content, embedding_vec FROM documents WHERE user_id = ?", (user_id,))
    for content, vec_blob in cur.fetchall():
        vec = safe_load_vector(vec_blob, "Document")
        if vec is not None:
            score = cosine_similarity(query_embedding, vec)
            all_scores.append((score, content))
    
    # 2b. 전체 채팅 로그 검색
    cur.execute("""
    SELECT c.message, c.embedding_vec 
    FROM chatlogs c
    WHERE c.user_id = ? AND (c.subject_id = ? OR ? IS NULL)
    """, (user_id, subject_id, subject_id))
    for message, vec_blob in cur.fetchall():
        vec = safe_load_vector(vec_blob, "Chatlog")
        if vec is not None:
            score = cosine_similarity(query_embedding, vec)
            all_scores.append((score, message))
    
    # 상위 k개 선택 및 RAG 컨텍스트 생성
    all_scores.sort(reverse=True, key=lambda x: x[0])
    top_contexts = [item[1] for item in all_scores[:top_k]]
    rag_context = "\n".join(top_contexts)
    
    cur.close()
    conn.close()
    
    return log_context, rag_context

# 쿼리와 응답을 chatlogs에 저장 (chatlogs 테이블 사용)
def store_chatlog(db_name: str, user_id: int, subject_id: Optional[int], role: str, message: str, embedding_vec: Optional[bytes] = None, is_target: bool = False):
    """대화 로그를 DB에 저장합니다."""
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    cur.execute("""
    INSERT INTO chatlogs (user_id, subject_id, role, message, embedding_vec, is_target)
    VALUES (?, ?, ?, ?, ?, ?)
    """, (user_id, subject_id, role, message, embedding_vec, is_target))
    log_id = cur.lastrowid
    conn.commit()
    cur.close()
    conn.close()
    return log_id

# 피드백 저장 (feedbacks 테이블 사용, 가상으로 rating/comment 저장)
def store_feedback(db_name: str, log_id: int, rating: int = 4, comment: str = "가상 피드백: 유용함"):
    """응답에 대한 피드백을 저장합니다."""
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    cur.execute("""
    INSERT INTO feedbacks (log_id, rating, comment)
    VALUES (?, ?, ?)
    """, (log_id, rating, comment))
    conn.commit()
    cur.close()
    conn.close()

# 사용자 정보 가져오기 (users 테이블 사용)
def get_user_info(db_name: str, user_id: int) -> str:
    """사용자 이름과 이메일을 검색합니다."""
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    cur.execute("SELECT name, email FROM users WHERE user_id = ?", (user_id,))
    result = cur.fetchone()
    cur.close()
    conn.close()
    return f"이름: {result[0]}, 이메일: {result[1]}" if result else "알 수 없음"

# LLM API와 소통하여 RAG 기반 응답 생성 (DB 사용)
def generate_rag_response(user_query: str, user_id: int, subject_name: Optional[str] = None, db_name: str = "llm_rag_db.sqlite") -> str:
    """
    사용자의 쿼리에 대해 DB에서 관련 로그/문서를 검색하고, Gemini API로 RAG 응답 생성.
    지수 백오프를 적용하여 API 호출 안정성 확보.
    """
    client = get_gemini_client()
    if client is None:
        return "AI 기능을 사용할 수 없습니다."
    
    # 사용자 정보 확인
    user_info = get_user_info(db_name, user_id)
    
    # 주제 ID 가져오기
    subject_id = None
    if subject_name:
        conn = sqlite3.connect(db_name)
        cur = conn.cursor()
        cur.execute("SELECT subject_id FROM subjects WHERE name = ?", (subject_name,))
        result = cur.fetchone()
        if result:
            subject_id = result[0]
        cur.close()
        conn.close()
    
    # 캐시 확인
    cached = get_cached_response(db_name, user_id, user_query)
    if cached:
        return cached + " (캐시된 응답)"
    
    try:
        # 쿼리 임베딩 생성
        query_embedding = get_embedding(user_query)
        # float32 배열을 바이트로 변환
        query_embedding_bytes = query_embedding.tobytes()
        
        # 쿼리 로그 저장 (is_target=True)
        store_chatlog(db_name, user_id, subject_id, "user", user_query, query_embedding_bytes, is_target=True)
        
        # DB에서 컨텍스트 검색
        log_context, rag_context = retrieve_context(db_name, user_id, query_embedding, subject_id)
        
        # 사용자 선호도 가져오기
        preferences = get_user_preferences(db_name, user_id)
        
        # 프롬프트 템플릿
        prompt_template = f"""
        당신은 데이터베이스 기반 RAG 시스템의 LLM 어시스턴트입니다.
        
        [사용자 정보]
        - 사용자 ID: {user_id}
        - 사용자 프로필: {user_info}
        - 주제: {subject_name or "일반"}
        - 선호도: {preferences}
        
        [RAG 컨텍스트]
        - 이전 대화 로그: {log_context}
        - 관련 문서 및 로그: {rag_context}
        
        [쿼리]
        - 사용자 쿼리: "{user_query}"
        
        작성 지침:
        1. 제공된 컨텍스트를 기반으로 사실에 충실한 응답을 생성합니다.
        2. 환각을 피하고, DB 데이터로 보강합니다.
        3. 응답은 3~5문장으로 간결하게 유지합니다. (선호도에 따라 조정: e.g., 'detailed' 시 더 상세히)
        4. 개인화: 사용자 선호도(예: 상세/간결)를 반영합니다.
        5. 끝에 피드백 요청을 추가합니다.
        """
        
        # 지수 백오프를 사용한 API 호출
        for attempt in range(MAX_RETRIES):
            try:
                response = client.generate_content(prompt_template)
                rag_response_text = response.text.strip()
                break
            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    delay = 2 ** attempt
                    print(f"RAG API 호출 실패 (시도 {attempt + 1}/{MAX_RETRIES}). {delay}초 후 재시도...")
                    time.sleep(delay)
                else:
                    raise e
        else:
            return "응답 생성 중 오류가 발생했습니다. (API 재시도 실패)"

        # 응답 임베딩 생성 및 저장
        response_embedding = get_embedding(rag_response_text)
        response_embedding_bytes = response_embedding.tobytes()
        log_id = store_chatlog(db_name, user_id, subject_id, "assistant", rag_response_text, response_embedding_bytes)
        
        # 캐시 저장
        store_cached_response(db_name, user_id, user_query, rag_response_text)
        
        # 가상 피드백 저장
        store_feedback(db_name, log_id)
        
        return rag_response_text
    except Exception as e:
        print(f"최종 오류: {e}")
        return "응답 생성 중 오류가 발생했습니다."

# DB를 사용하지 않은 일반 LLM 응답 생성
def generate_plain_response(user_query: str, user_id: int, subject_name: Optional[str] = None) -> str:
    """
    DB를 사용하지 않고 직접 Gemini API로 응답 생성. 지수 백오프 적용.
    """
    client = get_gemini_client()
    if client is None:
        return "AI 기능을 사용할 수 없습니다."
    
    # 간단한 프롬프트 템플릿 (컨텍스트 없이)
    prompt_template = f"""
    당신은 LLM 어시스턴트입니다.
    
    [사용자 정보]
    - 사용자 ID: {user_id}
    
    [쿼리]
    - 사용자 쿼리: "{user_query}"
    
    작성 지침:
    1. 사실에 충실한 응답을 생성합니다.
    2. 응답은 3~5문장으로 간결하게 유지합니다.
    3. 끝에 피드백 요청을 추가합니다.
    """
    
    # 지수 백오프를 사용한 API 호출
    for attempt in range(MAX_RETRIES):
        try:
            response = client.generate_content(prompt_template)
            return response.text.strip()
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                delay = 2 ** attempt
                print(f"일반 API 호출 실패 (시도 {attempt + 1}/{MAX_RETRIES}). {delay}초 후 재시도...")
                time.sleep(delay)
            else:
                print(f"API 오류: {e}")
                return "응답 생성 중 오류가 발생했습니다."
    return "응답 생성 중 오류가 발생했습니다. (API 재시도 실패)"

# 두 응답 비교하여 점수 계산 (TF-IDF 기반 cosine similarity)
def compare_responses(rag_response: str, plain_response: str) -> float:
    """
    RAG 응답과 일반 응답의 유사도를 계산하여 점수 반환 (0~1).
    """
    if not rag_response or not plain_response:
        return 0.0
    
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform([rag_response, plain_response])
    similarity = sk_cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
    return similarity

# 사용 예시 (테스트를 위해 사용자, 선호도, 주제 데이터 삽입)
if __name__ == "__main__":
    db_name = "llm_rag_db_test.sqlite" # 테스트 DB 파일명 변경
    
    # 이전 테스트 파일 삭제 및 DB 생성
    import os
    if os.path.exists(db_name):
        os.remove(db_name)
        print(f"✅ 이전 테스트 DB 파일 ({db_name}) 삭제 완료.")

    create_database_and_tables(db_name)

    # 테스트 데이터 삽입 (users, user_preferences, subjects, chatlogs, documents)
    conn = sqlite3.connect(db_name)
    cur = conn.cursor()
    
    # users
    cur.execute("INSERT INTO users (name, email) VALUES (?, ?)", ("Test User", "test@example.com"))
    user_id = cur.lastrowid
    
    # user_preferences
    cur.execute("INSERT INTO user_preferences (user_id, preference_type, value) VALUES (?, ?, ?)", (user_id, "style", "detailed"))
    
    # subjects
    cur.execute("INSERT INTO subjects (name, description) VALUES (?, ?)", ("DB 프로젝트", "데이터베이스 관련 주제"))
    cur.execute("SELECT subject_id FROM subjects WHERE name = ?", ("DB 프로젝트",))
    subject_id = cur.fetchone()[0]
    
    # chatlogs (5턴 샘플 데이터)
    print("\n--- 초기 로그 및 문서 임베딩 생성 및 저장 ---")
    embedding1 = get_embedding('안녕하세요, LLM에 대해 알려주세요.').tobytes()
    embedding2 = get_embedding('LLM은 대형 언어 모델로, 자연어 처리를 합니다.').tobytes()
    embedding3 = get_embedding('RAG 시스템은 어떻게 작동하나요?').tobytes()
    embedding4 = get_embedding('RAG은 검색 증강 생성으로, 외부 데이터를 검색해 응답을 보강합니다.').tobytes()
    embedding5 = get_embedding('데이터베이스 통합의 이점은?').tobytes()
    
    cur.execute("""
    INSERT INTO chatlogs (user_id, subject_id, role, message, embedding_vec, is_target) 
    VALUES (?, ?, 'user', '안녕하세요, LLM에 대해 알려주세요.', ?, 1)
    """, (user_id, subject_id, embedding1))
    cur.execute("""
    INSERT INTO chatlogs (user_id, subject_id, role, message, embedding_vec, is_target) 
    VALUES (?, ?, 'assistant', 'LLM은 대형 언어 모델로, 자연어 처리를 합니다.', ?, 0)
    """, (user_id, subject_id, embedding2))
    cur.execute("""
    INSERT INTO chatlogs (user_id, subject_id, role, message, embedding_vec, is_target) 
    VALUES (?, ?, 'user', 'RAG 시스템은 어떻게 작동하나요?', ?, 1)
    """, (user_id, subject_id, embedding3))
    cur.execute("""
    INSERT INTO chatlogs (user_id, subject_id, role, message, embedding_vec, is_target) 
    VALUES (?, ?, 'assistant', 'RAG은 검색 증강 생성으로, 외부 데이터를 검색해 응답을 보강합니다.', ?, 0)
    """, (user_id, subject_id, embedding4))
    cur.execute("""
    INSERT INTO chatlogs (user_id, subject_id, role, message, embedding_vec, is_target) 
    VALUES (?, ?, 'user', '데이터베이스 통합의 이점은?', ?, 1)
    """, (user_id, subject_id, embedding5))

    # documents (샘플 데이터 추가: 2개 문서)
    doc_embedding1 = get_embedding('데이터베이스 설계는 데이터를 구조화하고 무결성을 보장하는 과정입니다.').tobytes()
    doc_embedding2 = get_embedding('RAG 시스템은 LLM의 정확성을 높이며, 최신 정보 접근을 가능하게 합니다.').tobytes()
    cur.execute("""
    INSERT INTO documents (user_id, content, embedding_vec) 
    VALUES (?, '데이터베이스 설계는 데이터를 구조화하고 무결성을 보장하는 과정입니다.', ?)
    """, (user_id, doc_embedding1))
    cur.execute("""
    INSERT INTO documents (user_id, content, embedding_vec) 
    VALUES (?, 'RAG 시스템은 LLM의 정확성을 높이며, 최신 정보 접근을 가능하게 합니다.', ?)
    """, (user_id, doc_embedding2))
    
    conn.commit()
    cur.close()
    conn.close()
    
    print("--- 테스트 실행 ---")
    
    # 테스트 쿼리
    user_query = "데이터베이스 설계에 대해 알려줘"
    
    # RAG 응답 (DB 사용)
    rag_response = generate_rag_response(user_query, user_id, "DB 프로젝트", db_name)
    print("\n[RAG 응답]:", rag_response)
    
    # 일반 응답 (DB 미사용)
    plain_response = generate_plain_response(user_query, user_id)
    print("\n[일반 응답]:", plain_response)
    
    # 비교 점수
    score = compare_responses(rag_response, plain_response)
    print(f"\n[비교 점수]: 두 응답의 유사도 점수: {score:.2f}")
